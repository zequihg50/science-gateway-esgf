{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4113efc-76a8-42a4-8fef-ae1c0e96caeb",
   "metadata": {},
   "source": [
    "# ESGF Virtual Aggregation - Projected climate change signals and uncertainty under global warming levels\n",
    "\n",
    "> This notebook is a reproducibility example of the IPCC-WGI AR6 Interactive Atlas products, which has been adapted to work with the ESGF Virtual Aggregation. This work is licensed under a [Creative Commons Attribution 4.0 International License](http://creativecommons.org/licenses/by/4.0).\n",
    ">\n",
    "> ![Creative Commons License >](https://i.creativecommons.org/l/by/4.0/88x31.png)\n",
    "\n",
    "**E. Cimadevilla** (Santander Meteorology Group. Instituto de Física de Cantabria, CSIC-UC, Santander, Spain).\n",
    "\n",
    "This notebook is an **example of the calculation and visualization of the IPCC-WGI AR6 uncertainty methods (simple and advanced) for projected delta changes**. Please refer to the **AR6 WGI Cross-Chapter Box Atlas 1** ([Gutiérrez et al., 2021](https://www.ipcc.ch/report/ar6/wg1/chapter/atlas/)) for more information. We also **introduce the Global Warming Level dimension for the analysis of the climate change signal**.\n",
    "\n",
    "This notebook works with data available in the ESGF, which is accessed in a **remote data access** fashion through virtual aggregations provided by the [ESGF Virtual Aggregation](https://doi.org/10.5194/gmd-2024-120)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8031d32-f077-4838-9735-f9bdbc44d50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cftime\n",
    "\n",
    "import xarray\n",
    "import dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9030fef6-adad-4d69-bdd9-df681319bfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.config.set(scheduler=\"processes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9446c5c6-9634-4573-9891-9a01e49bed65",
   "metadata": {},
   "source": [
    "## 1. The Global Warming Level analysis dimension\n",
    "\n",
    "Instead of calculating climate change anomalies for a fixed period, we will calculate the anomaly for a given level of global warming (GWL). To do so, we need the information on the **time windows where the global surface temperature reaches the different levels of warming**. This information is available at the [IPCC-WGI/Atlas GitHub repository](https://github.com/IPCC-WG1/Atlas/tree/main/notebooks).\n",
    "\n",
    "Note that we have pointed to the `CMIP6_Atlas_WarmingLevels.csv` file, as we are going to work with CMIP6 data. In this example, we will focus on the +3ºC GWL. We will consider the ssp585 scenario, however, any other scenario can be considered, as the anomalies do not vary significantly across scenarios when the GWL dimension is analyzed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb7dee1-f377-4a43-a303-bf668b8664c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://github.com/SantanderMetGroup/ATLAS/raw/refs/heads/main/warming-levels/CMIP6_Atlas_WarmingLevels.csv\"\n",
    "gwls_file = url.split(\"/\")[-1]\n",
    "with open(gwls_file, \"w\") as f:\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    f.write(response.text)\n",
    "gwls = pd.read_csv(gwls_file)\n",
    "model_runs = gwls[\"model_run\"].str.split(\"_\", expand=True).set_index(0)[1].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcb89f3-d2ba-4d10-95aa-ee3813189658",
   "metadata": {},
   "source": [
    "## 2. Data loading for the different GWLs\n",
    "\n",
    "To locate the data in which we are interested, we use the CSV inventory of the ESGF Virtual Aggregation. We will analyze the inventory using standard Pandas functionality. The data will be loaded in a remote data access fashion via OPeNDAP provided by a THREDDS Data Server.\n",
    "\n",
    "The versions of the ESGF datasets in which we are interested are available in the `atlas-pr.csv' file provided in this same directory. These versions were extracted from the [IPCC-WGI/Atlas GitHub repository](https://raw.githack.com/IPCC-WG1/Atlas/devel/data-sources/CMIP6/CMIP6.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a847ab-4674-4f73-ac20-bd440484e663",
   "metadata": {},
   "outputs": [],
   "source": [
    "esgf_va_catalog = pd.read_csv(\"esgf-va_catalog.csv.zip\").fillna(\"\")\n",
    "atlas_ipcc_catalog = pd.read_csv(\"atlas-pr.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1490316-8814-4e0c-8469-9db652500571",
   "metadata": {},
   "source": [
    "The following function manages the locations of the datasets from the ESGF Virtual Aggregation that we are interested in. Since some versions are no longer available on the ESGF—having been replaced by newer versions—not all of the original datasets will be loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed175153-bb7f-4772-86f1-ea4c58edd57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_model_runs(esgf_va_catalog, atlas_ipcc_catalog, model_runs, experiment):\n",
    "    ignored_models = [\n",
    "        \"EC-Earth3-Veg\", # replica unavailable\n",
    "        \"BCC-CSM2-MR\", # time units change across netCDF files\n",
    "        \"KACE-1-0-G\", # there is no ssp585 for this model\n",
    "        \"UKESM1-0-LL\", # there is no ssp585 for this model\n",
    "        \"MIROC-ES2L\", # no replica seems to work\n",
    "\n",
    "        # not available later for Pangeo\n",
    "        \"ACCESS-CM2\",\n",
    "        \"ACCESS-ESM1-5\",\n",
    "\n",
    "        # temporal removes\n",
    "        \"KIOST-ESM\",\n",
    "    ]\n",
    "    \n",
    "    preferred_replicas = {\n",
    "        \"historical\": {\n",
    "            \"EC-Earth3-Veg\": \"esgf3.dkrz.de\",\n",
    "            \"HadGEM3-GC31-LL\": \"\", # esgf-data04.diasjp.net netcdf-java error, ...\n",
    "        },\n",
    "        \"ssp585\": {\n",
    "            \"EC-Earth3-Veg\": \"esgf3.dkrz.de\",\n",
    "            \"HadGEM3-GC31-LL\": \"\",\n",
    "        },\n",
    "    }\n",
    "    \n",
    "    esgf_va_model_runs = []\n",
    "    for model in model_runs:\n",
    "        if model in ignored_models:\n",
    "            continue\n",
    "    \n",
    "        esgf_va_subset = esgf_va_catalog.query(f\"project == 'CMIP6' & model == '{model}' & table == 'day' & variable == 'pr' & experiment == '{experiment}'\").set_index([\"project\", \"model\", \"experiment\", \"version\"]).drop([\"variable\"], axis=1)\n",
    "        atlas_ipcc_subset = atlas_ipcc_catalog.drop([\"variable\"], axis=1)\n",
    "        \n",
    "        subset = esgf_va_subset.join(atlas_ipcc_subset.set_index([\"project\", \"model\", \"experiment\", \"version\"]), on=[\"project\", \"model\", \"experiment\", \"version\"], how=\"inner\")\n",
    "        if len(subset) > 0:\n",
    "            # Choose the preferred replica or default to esgf.ceda.ac.uk\n",
    "            if model in preferred_replicas[experiment]:\n",
    "                replica = preferred_replicas[experiment][model]\n",
    "            else:\n",
    "                replica = \"esgf.ceda.ac.uk\"\n",
    "    \n",
    "            # Probe if the required model member is included in the esgf-va\n",
    "            url = subset[subset[\"replica\"] == replica][\"location\"].iloc[0]\n",
    "            print(url)\n",
    "            ds = xarray.open_dataset(\n",
    "                url,\n",
    "                decode_times=False,\n",
    "                drop_variables=[\"tracking_id\", \"further_info_url\", \"time_bnds\", \"lat_bnds\", \"lon_bnds\"])\n",
    "    \n",
    "            if model_runs[model].encode(\"ascii\") in ds[\"variant_label\"].values:\n",
    "                record = {\"url\": url, \"model_run\": model + \"_\" + model_runs[model]}\n",
    "                esgf_va_model_runs.append(record)\n",
    "\n",
    "    return pd.DataFrame.from_records(esgf_va_model_runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829cedba-5041-4f60-816b-2b31027af9f3",
   "metadata": {},
   "source": [
    "Locate the datasets for both the historical and future scenario periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7959044-73bb-4f8b-92e1-14962d5b86e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssp585_model_runs = find_model_runs(esgf_va_catalog, atlas_ipcc_catalog, model_runs, \"ssp585\")\n",
    "historical_model_runs = find_model_runs(esgf_va_catalog, atlas_ipcc_catalog, model_runs, \"historical\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681e7aa8-18fc-4d14-b5cb-9250e6867e7c",
   "metadata": {},
   "source": [
    "We now create a DataFrame containing all the available datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de067f13-5dcf-438f-bb1c-abaded33fdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = historical_model_runs.set_index(\"model_run\").join(\n",
    "    ssp585_model_runs.set_index(\"model_run\"), how=\"inner\", lsuffix=\"h\", rsuffix=\"s\").rename(\n",
    "    {\"urlh\": \"historical\", \"urls\": \"ssp585\"}, axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f72311-a5af-45ac-9dab-abeb9069f3b0",
   "metadata": {},
   "source": [
    "Store the inventory of URLs for future usages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25466d54-4695-4389-839c-b46483ce346c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(\"esgf-va_pr_ceda.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b64de6-6c8d-483f-b969-e3ec2338785b",
   "metadata": {},
   "source": [
    "Open the datasets using [xarray](https://docs.xarray.dev/en/stable/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6e6189-766c-4424-b73b-bd0666415426",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets(locations, members, experiment):\n",
    "    dss = []\n",
    "    for location in locations:\n",
    "        name = location.split(\"/\")[-1]\n",
    "        model = name.split(\"_\")[3]\n",
    "\n",
    "        ds = xarray.open_dataset(\n",
    "            location,\n",
    "            drop_variables=[\"tracking_id\", \"further_info_url\", \"time_bnds\", \"lat_bnds\", \"lon_bnds\"]).sel(\n",
    "            variant_label=members[model].encode(\"ascii\"))\n",
    "\n",
    "        if experiment == \"ssp585\":\n",
    "            ds = ds.isel(time=slice(None, 31390))\n",
    "            # limit to 2100, avoid cf time indexing issues\n",
    "            if isinstance(ds[\"time\"][0].item(), cftime.Datetime360Day):\n",
    "                ds = ds.sel(time=slice(\"20150101\", \"21001230\"))\n",
    "            else:\n",
    "                ds = ds.sel(time=slice(\"20150101\", \"21001231\"))\n",
    "        elif experiment == \"historical\":\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(\"Invalid experiment... exiting\")\n",
    "    \n",
    "        # add the model_run global attribute\n",
    "        ds.attrs[\"model_run\"] = f\"{model}_{members[model]}\"\n",
    "        dss.append(ds)\n",
    "\n",
    "    return dss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecbfced-3226-4ec3-a1f0-e1515c4efffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_dss = load_datasets(df[\"historical\"], model_runs, \"historical\")\n",
    "ssp585_dss = load_datasets(df[\"ssp585\"], model_runs, \"ssp585\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f82df0-8a06-4c28-8181-4408764bc29c",
   "metadata": {},
   "source": [
    "Print size of the datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caab10d0-38af-41f1-adf2-9edb0ca4673a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds in historical_dss:\n",
    "    print(f'{ds[\"pr\"].size * ds[\"pr\"].dtype.itemsize / 2**20:.2f} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0568db-74a4-4262-bfdd-4afd46180e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds in ssp585_dss:\n",
    "    print(f'{ds[\"pr\"].size * ds[\"pr\"].dtype.itemsize / 2**20:.2f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16868ee7-8567-47a7-8378-b33bf866bae4",
   "metadata": {},
   "source": [
    "## 3. Data retrieval and subsetting\n",
    "\n",
    "From the previous section, we observe that the datasets are relatively large, which would require substantial data transfer from the ESGF data nodes to the client executing the climate data analysis workflow. However, since we are focusing only on the European region, we can take advantage of remote data access subsetting capabilities to retrieve only the necessary data from the ESGF nodes to support the workflow.\n",
    "\n",
    "To achieve this, we will implement a function responsible for efficiently fetching the required data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ed6758-25c9-4621-a8b7-658a4d1006d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def efficient_retrieval_and_compute(ds):\n",
    "    lats, lons = slice(35, 72), slice(-11, 35) # Europe in (-180, 180)\n",
    "\n",
    "    # handle Europe being split when using (0, 360)\n",
    "    if ds[\"lon\"].min().item() >= 0:\n",
    "        left = ds.sel(lon=slice(0, 36), lat=lats).chunk({\"time\": 1000})\n",
    "        right = ds.sel(lon=slice(348, 360), lat=lats).chunk({\"time\": 1000})\n",
    "\n",
    "        spatial = xarray.concat([left, right], dim=\"lon\")\n",
    "    else:\n",
    "        spatial = ds.sel(lon=lons, lat=lats)\n",
    "\n",
    "    winter = spatial.sel(time=spatial.time.dt.month.isin([12,1,2])).resample({\"time\": \"MS\"}).sum().load()\n",
    "    winter = winter.assign_coords(lon=((winter[\"lon\"] + 180) % 360) - 180).sortby(\"lon\")\n",
    "\n",
    "    return winter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d83cb1-4738-4050-a2cc-059045211d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "historical_dss_subset = [efficient_retrieval_and_compute(ds) for ds in historical_dss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bc332d-7bcb-414f-b09a-3ac05ff3c2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ssp585_dss_subset = [efficient_retrieval_and_compute(ds) for ds in ssp585_dss]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f039321c-e0c8-46d9-ac22-e9a11429d2d2",
   "metadata": {},
   "source": [
    "Now that we have loaded all the required data via subsetting and remote data access, we can examine the actual amount of data needed to carry out this workflow. We can observe that the data required to carry out the workflow is much smaller compared to the full size of the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3db9903-5883-45bf-9367-731bed957fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds in historical_dss_subset:\n",
    "    print(f'{ds[\"pr\"].size * ds[\"pr\"].dtype.itemsize / 2**20:.2f} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe10cab-45b8-44a4-921b-cd7c1d8607eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds in ssp585_dss_subset:\n",
    "    print(f'{ds[\"pr\"].size * ds[\"pr\"].dtype.itemsize / 2**20:.2f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cb0819-93b5-4487-9661-1c64d72827a0",
   "metadata": {},
   "source": [
    "## 4. Interpolation to a common grid\n",
    "\n",
    "To perform the model agreement task, we need to interpolate to the same grid of the original Interactive Atlas dataset. We will obtain the grid from the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d98d6b-68cd-494e-b886-185a1a5875ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "iad_catalog = pd.read_csv(\"https://raw.githubusercontent.com/SantanderMetGroup/IPCC-Atlas-Datalab/refs/heads/main/data_inventory.csv\")\n",
    "iad_subset = iad_catalog.query('type == \"opendap\" & variable == \"pr\" & project == \"CMIP6\" & experiment == \"ssp585\" & frequency == \"mon\"')\n",
    "iad_location = iad_subset[\"location\"].iloc[0]\n",
    "iad_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e0dd75-7edf-4122-a1de-df82b092afed",
   "metadata": {},
   "outputs": [],
   "source": [
    "iad = xarray.open_dataset(iad_location).sel(lat=slice(35, 72), lon=slice(-11, 35))\n",
    "iad[\"member\"] = [b\"_\".join(member.split(b\"_\")[1:]).decode(\"ascii\") for member in iad[\"member\"].values]\n",
    "iad[\"pr\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53918972-0bcc-4c91-84f6-bfdd11fa84db",
   "metadata": {},
   "source": [
    "Once we have got access to the Interactive Atlas dataset, we perform interpolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3109a11-32f6-4117-a224-686c7345e60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssp585_dss_subset_interp = [ds.interp(\n",
    "    lat=iad[\"lat\"],\n",
    "    lon=iad[\"lon\"],\n",
    "    method=\"linear\",\n",
    "    kwargs={\"fill_value\": \"extrapolate\"})\n",
    "              for ds in ssp585_dss_subset]\n",
    "\n",
    "historical_dss_subset_interp = [ds.interp(\n",
    "    lat=iad[\"lat\"],\n",
    "    lon=iad[\"lon\"],\n",
    "    method=\"linear\",\n",
    "    kwargs={\"fill_value\": \"extrapolate\"})\n",
    "              for ds in historical_dss_subset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684a14e0-fb67-4b36-bf10-5e51c86476da",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds in ssp585_dss_subset_interp:\n",
    "    ds[\"time\"] = ds[\"time\"].astype(\"datetime64[ns]\")\n",
    "    ds[\"pr\"] = xarray.where(ds[\"pr\"] < 0, 0, ds[\"pr\"])\n",
    "\n",
    "for ds in historical_dss_subset_interp:\n",
    "    ds[\"time\"] = ds[\"time\"].astype(\"datetime64[ns]\")\n",
    "    ds[\"pr\"] = xarray.where(ds[\"pr\"] < 0, 0, ds[\"pr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2320f79-d33a-4eb3-9b55-f93a70623c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmip6_ssp585 = xarray.concat(\n",
    "    ssp585_dss_subset_interp,\n",
    "    xarray.Variable(\"member\", [ds.attrs[\"model_run\"] for ds in ssp585_dss_subset_interp]))\n",
    "\n",
    "cmip6_historical = xarray.concat(\n",
    "    historical_dss_subset_interp,\n",
    "    xarray.Variable(\"member\", [ds.attrs[\"model_run\"] for ds in historical_dss_subset_interp]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd266c7-136c-4ae7-9d35-3457d31e436a",
   "metadata": {},
   "source": [
    "## 5. Global warming levels\n",
    "\n",
    "Since we are working with warming levels and need to load a different period of years for each model, we will first extract the information about the members from the inventory. This information is essential to match the GWL array obtained earlier. In other words, we need to create an index object that provides the model position in the data for each row in the gwl3 object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adee5912-790a-43df-8572-a138fb6368ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "gwls3 = gwls[[\"model_run\", \"3_ssp585\"]]\n",
    "model_run_gwl = df.join(gwls3.set_index(\"model_run\"), on=\"model_run\")[\"3_ssp585\"]\n",
    "model_run_gwl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cb305a-8614-47ff-b6a0-ef9534499a06",
   "metadata": {},
   "source": [
    "Extract a 20-year period for each model: 10 years before and 10 years after the model reaches the GWL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5553d4-5cca-460a-8114-1f88a0dc406f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmip6_ssp585_gwl = xarray.concat(\n",
    "    [cmip6_ssp585.sel(\n",
    "        member=model_run,\n",
    "        time=slice(f\"{year-10}1201\", f\"{year+10}0201\"))\n",
    "     for model_run, year in model_run_gwl.reset_index().values\n",
    "     if year != 9999],\n",
    "    \"member\") * 2880"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7846efc-04d2-45a1-b542-206d36cebff0",
   "metadata": {},
   "source": [
    "We perform an annual aggregation of the variable. Additionally, we generate a time-series plot to illustrate the concept of GWLs, where each model spans a different time period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f90c022-c287-4f72-9dcc-cf35e747ce15",
   "metadata": {},
   "outputs": [],
   "source": [
    "qmean = cmip6_ssp585_gwl.mean([\"lat\", \"lon\"]).resample(time=\"QS-DEC\").mean()\n",
    "plot = qmean[\"pr\"].sel(time=qmean[\"pr\"].time.dt.month==12).plot.line(x=\"time\", add_legend=False, figsize=(15, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e204a3bb-9b42-4664-8611-32ff1dfee828",
   "metadata": {},
   "source": [
    "Now we plot the map of the climatologies (mean of the period; 20-year means in this case). In the resulting plot, each panel represents a CMIP6 member (a model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed8d0cc-70f4-432b-b93f-b15e9ae0cffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "lats, lons = slice(35, 72), slice(-11, 35)\n",
    "\n",
    "plot = cmip6_ssp585_gwl[\"pr\"].mean(\"time\").plot(\n",
    "    x=\"lon\", y=\"lat\", col=\"member\", col_wrap=6,\n",
    "    # vmin=0, vmax=13,\n",
    "    figsize=(28,13),\n",
    "    add_colorbar=True,\n",
    "    cmap=\"coolwarm_r\",\n",
    "    cbar_kwargs={\"shrink\": .5},\n",
    "    subplot_kws=dict(projection=ccrs.PlateCarree(central_longitude=0)),\n",
    "    transform=ccrs.PlateCarree())\n",
    "\n",
    "for ax in plot.axs.flatten():\n",
    "    ax.coastlines()\n",
    "    ax.set_extent((lons.start, lons.stop, lats.start, lats.stop), ccrs.PlateCarree())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ffa517-556f-4a7a-b316-09f8d070ba97",
   "metadata": {},
   "source": [
    "To get the climate change signal, we first need to load data from the historical scenario. To do this we repeat the filtering process to get the required dataset.\n",
    "\n",
    "We are considering the pre-industrial period. In this case, the reference period is common to all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49be13d1-0c7b-4f51-a17d-bafde01a30a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmip6_historical_gwl = xarray.concat(\n",
    "    [cmip6_historical.sel(\n",
    "        member=model_run,\n",
    "        time=slice(\"18501201\", \"19000201\"))\n",
    "     for model_run, year in model_run_gwl.reset_index().values\n",
    "     if year != 9999],\n",
    "    \"member\") * 2880"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9f319d-056a-4632-a1fb-60e85681c53d",
   "metadata": {},
   "source": [
    "Now we can calculate the anomaly by computing the difference between both climatologies. We can easily do this using xarray functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5608464b-41f9-43f2-8774-9ae5edceeb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = cmip6_ssp585_gwl[\"pr\"].mean(\"time\") - cmip6_historical_gwl[\"pr\"].mean(\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156c609f-4d8a-478e-a9ae-25c916f04895",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = delta.plot(\n",
    "    x=\"lon\", y=\"lat\", col=\"member\", col_wrap=6,\n",
    "    figsize=(28,13),\n",
    "    add_colorbar=True,\n",
    "    cmap=\"coolwarm_r\",\n",
    "    cbar_kwargs={\"shrink\": .5},\n",
    "    subplot_kws=dict(projection=ccrs.PlateCarree(central_longitude=0)),\n",
    "    transform=ccrs.PlateCarree())\n",
    "\n",
    "for ax in plot.axs.flatten():\n",
    "    ax.coastlines()\n",
    "    ax.set_extent((lons.start, lons.stop, lats.start, lats.stop), ccrs.PlateCarree())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c561e1-ed70-442c-8312-a5c648af2ca2",
   "metadata": {},
   "source": [
    "These maps represent the climate change signal as an absolute anomaly. We can calculate the relative anomaly as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc630fd5-983a-441f-b1d8-04d7c9846a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_rel = (delta / cmip6_historical_gwl[\"pr\"].mean(\"time\")) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97b83b1-89e2-4a5c-890f-f971d30406b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = delta_rel.plot(\n",
    "    x=\"lon\", y=\"lat\", col=\"member\", col_wrap=6,\n",
    "    figsize=(28,13),\n",
    "    vmax=200, vmin=-200,\n",
    "    add_colorbar=True,\n",
    "    cmap=\"BrBG\",\n",
    "    cbar_kwargs={\"shrink\": .5},\n",
    "    subplot_kws=dict(projection=ccrs.PlateCarree(central_longitude=0)),\n",
    "    transform=ccrs.PlateCarree())\n",
    "\n",
    "for ax in plot.axs.flatten():\n",
    "    ax.coastlines()\n",
    "    ax.set_extent((lons.start, lons.stop, lats.start, lats.stop), ccrs.PlateCarree())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f0d365-c670-48cf-a087-702a8869682d",
   "metadata": {},
   "source": [
    "We now calculate and plot the multi-model anomaly mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e04677d-cf24-4593-afcb-8d228a3244df",
   "metadata": {},
   "outputs": [],
   "source": [
    "ens_mean = delta_rel.mean(\"member\")\n",
    "\n",
    "plot = ens_mean.plot(\n",
    "    add_colorbar=True,\n",
    "    cmap=\"BrBG\",\n",
    "    cbar_kwargs={\"shrink\": .5},\n",
    "    vmin=-50, vmax=50,\n",
    "    subplot_kws=dict(projection=ccrs.PlateCarree(central_longitude=0)),\n",
    "    transform=ccrs.PlateCarree())\n",
    "\n",
    "plot.axes.coastlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65eccfbf-6546-462c-a419-df1ebdced235",
   "metadata": {},
   "source": [
    "## 6. Uncertainty calculation and representation\n",
    "\n",
    "We implement both the \"simple\" and \"advanced\" methods for uncertainty characterization defined in the IPCC Sixth Assessment Report. Please refer to the AR6 WGI Cross-Chapter Box Atlas 1 (Gutiérrez et al., 2021) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdb6334-88dd-487c-8599-d4647107e88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_agreement(da, axis, th = 80):\n",
    "    nmembers, nlat, nlon = da.shape\n",
    "    mask = np.array([\n",
    "        (da[:,i,j] > 0).sum() > int(nmembers * th / 100)\n",
    "        if da[:,i,j].mean() > 0\n",
    "        else (da[:,i,j] < 0).sum() > int(nmembers * th / 100)\n",
    "        for i in range(nlat)\n",
    "        for j in range(nlon)]).reshape((nlat,nlon))\n",
    "    \n",
    "    return mask\n",
    "\n",
    "def hatching(plot, mask, data):\n",
    "    rows, cols = mask.shape\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            lat, lon = data[\"lat\"][i].item(), data[\"lon\"][j].item()\n",
    "            if mask[i,j]:\n",
    "                plot.axes.plot([lon-.5,lon+.5],[lat+.5,lat-.5],'-',c=\"black\", linewidth=.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e52c577-f258-4ffa-8593-861764ad866d",
   "metadata": {},
   "source": [
    "Compute and display uncertainty via hatching using the \"simple\" method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9774a9c-aab7-4431-a65b-e7e018fce302",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_simple = delta.reduce(model_agreement, \"member\")\n",
    "\n",
    "plot = ens_mean.plot(\n",
    "    add_colorbar=True,\n",
    "    cmap=\"BrBG\",\n",
    "    cbar_kwargs={\"shrink\": .5},\n",
    "    vmin=-50, vmax=50,\n",
    "    subplot_kws=dict(projection=ccrs.PlateCarree(central_longitude=0)),\n",
    "    transform=ccrs.PlateCarree())\n",
    "\n",
    "hatching(plot, ~mask_simple, ens_mean)\n",
    "\n",
    "plot.axes.set_title(f\"ESGF-VA ({len(delta['member'])} models)\")\n",
    "plot.axes.coastlines()\n",
    "\n",
    "plt.savefig(\"uncer-europe-esgfva.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cf85f6-cdc4-4990-9f54-b61a56f83544",
   "metadata": {},
   "source": [
    "We now display regional precipitation stripes for Europe. Each model represents a 20-year period centered on the point at which it reaches the GWL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e69ac3-3986-4ba3-924e-f09975cec050",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmip6_ssp585_y = cmip6_ssp585_gwl[\"pr\"].resample(time=\"QS-DEC\").mean()\n",
    "cmip6_ssp585_y = cmip6_ssp585_y.sel(time=cmip6_ssp585_y[\"time\"].dt.month==12)\n",
    "\n",
    "cmip6_hist_c = cmip6_historical_gwl[\"pr\"].mean([\"time\"])\n",
    "\n",
    "year_delta = cmip6_ssp585_y - cmip6_hist_c\n",
    "year_delta_rel = year_delta / cmip6_hist_c * 100\n",
    "\n",
    "regional_mean = year_delta_rel.mean([\"lat\", \"lon\"])\n",
    "regional_mean[\"member\"] = [x.split(\"_\")[0] for x in regional_mean[\"member\"].values]\n",
    "\n",
    "regional_mean.plot.imshow(\n",
    "    figsize=(14,3.5),\n",
    "    add_colorbar=True,\n",
    "    cmap=\"BrBG\",\n",
    "    vmin=-50,vmax=50)\n",
    "plt.savefig(\"regional-stripes-pr.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5ed4d8-55e8-4aee-a62b-8700d26cefcc",
   "metadata": {},
   "source": [
    "## 7. Pangeo\n",
    "\n",
    "Until now, we have been working with the datasets provided by the ESGF Virtual Aggregation. We now carry out the same climate data analysis workflow by using the Analysis-Ready Cloud-Optimized datasets provided by [Pangeo](https://doi.org/10.1109/MCSE.2021.3059437)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bfb64d-260c-44ad-8ac9-16c6f63b23fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pangeo_catalog = pd.read_csv('https://storage.googleapis.com/cmip6/cmip6-zarr-consolidated-stores.csv')\n",
    "pangeo_catalog[\"version\"] = pangeo_catalog[\"version\"].astype(str)\n",
    "pangeo_catalog.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2feedc7-b65e-46f5-91e4-ac857c34ff5e",
   "metadata": {},
   "source": [
    "Find the same model runs used within the ESGF Virtual Aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ee92a2-772d-4695-9e6e-0641caf57070",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_model_runs_pangeo(pangeo_catalog, atlas_ipcc_catalog, model_runs, esgf_va_df, experiment):\n",
    "    pangeo_model_runs = []\n",
    "    \n",
    "    for model, run in esgf_va_df.index.str.split(\"_\"):\n",
    "        version = atlas_ipcc_catalog.query(f\"model == '{model}' & experiment == '{experiment}' & member == '{run}' & variable == 'pr'\")[\"version\"].iloc[0][1:]\n",
    "        subset = pangeo_catalog.query(f\"source_id == '{model}' & table_id == 'day' & variable_id == 'pr' & experiment_id == '{experiment}' & member_id == '{run}' & version == '{version}'\").set_index([\"source_id\", \"experiment_id\", \"version\"]).drop([\"variable_id\"], axis=1)\n",
    "\n",
    "        if model in [\"ACCESS-CM2\",\"ACCESS-ESM1-5\"]:\n",
    "            continue\n",
    "\n",
    "        url = subset[\"zstore\"].iloc[0]\n",
    "        record = {\"url\": url, \"model_run\": model + \"_\" + model_runs[model]}\n",
    "        pangeo_model_runs.append(record)\n",
    "\n",
    "    return pd.DataFrame.from_records(pangeo_model_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1759fc61-c93e-45a1-9947-b4326c96e2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssp585_model_runs_pangeo = find_model_runs_pangeo(pangeo_catalog, atlas_ipcc_catalog, model_runs, df, \"ssp585\")\n",
    "historical_model_runs_pangeo = find_model_runs_pangeo(pangeo_catalog, atlas_ipcc_catalog, model_runs, df, \"historical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a43175-9d5d-494d-ba74-75c4450a22f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pangeo = historical_model_runs_pangeo.set_index(\"model_run\").join(ssp585_model_runs_pangeo.set_index(\"model_run\"), how=\"inner\", lsuffix=\"h\", rsuffix=\"s\").rename({\"urlh\": \"historical\", \"urls\": \"ssp585\"}, axis=1)\n",
    "df_pangeo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9a7be5-735e-4ab7-a3c0-71502188d87e",
   "metadata": {},
   "source": [
    "Load the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0cdaa6-bc92-4920-bbc8-912f64dd8126",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets_pangeo(locations, members, experiment):\n",
    "    dss = []\n",
    "    for location in locations:\n",
    "        model = location.split(\"/\")[6]\n",
    "\n",
    "        ds = xarray.open_zarr(\n",
    "            location,\n",
    "            drop_variables=[\"time_bnds\", \"lat_bnds\", \"lon_bnds\"])\n",
    "\n",
    "        if experiment == \"ssp585\":\n",
    "            ds = ds.isel(time=slice(None, 31390))\n",
    "            # limit to 2100, avoid cf time indexing issues\n",
    "            if isinstance(ds[\"time\"][0].item(), cftime.Datetime360Day):\n",
    "                ds = ds.sel(time=slice(\"20150101\", \"21001230\"))\n",
    "            else:\n",
    "                ds = ds.sel(time=slice(\"20150101\", \"21001231\"))\n",
    "        elif experiment == \"historical\":\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(\"Invalid experiment... exiting\")\n",
    "    \n",
    "        # add the model_run global attribute\n",
    "        ds.attrs[\"model_run\"] = f\"{model}_{members[model]}\"\n",
    "        dss.append(ds)\n",
    "\n",
    "    return dss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fa7679-6d86-4424-bbc5-dbe1dce839f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_dss_pangeo = load_datasets_pangeo(df_pangeo[\"historical\"], model_runs, \"historical\")\n",
    "ssp585_dss_pangeo = load_datasets_pangeo(df_pangeo[\"ssp585\"], model_runs, \"ssp585\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff15a17e-93c2-4ba7-ab21-e386d8b2ce6a",
   "metadata": {},
   "source": [
    "Print size of the datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09488850-6790-4666-a329-05b9126e8250",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds in historical_dss_pangeo:\n",
    "    print(f'{ds[\"pr\"].size * ds[\"pr\"].dtype.itemsize / 2**20:.2f} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8568eda-0832-446f-9d11-899a4b5fde7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds in ssp585_dss_pangeo:\n",
    "    print(f'{ds[\"pr\"].size * ds[\"pr\"].dtype.itemsize / 2**20:.2f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296dd1b1-2883-40b0-8302-68ab1422bf2e",
   "metadata": {},
   "source": [
    "Implement the function that will subset and fetch the data from the cloud repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e388d50-7f53-4c45-b46c-4fc58c15beea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def efficient_retrieval_and_compute_pangeo(ds):\n",
    "    lats, lons = slice(35, 72), slice(-11, 35) # Europe in (-180, 180)\n",
    "\n",
    "    # handle Europe being split when using (0, 360)\n",
    "    if ds[\"lon\"].min().item() >= 0:\n",
    "        spatial = ds.sel(\n",
    "            time=ds.time.dt.month.isin([12,1,2]),\n",
    "            lon=(ds[\"lon\"] <= 36) | (ds[\"lon\"] >= 348),\n",
    "            lat=slice(35, 72)).load()\n",
    "    else:\n",
    "        spatial = ds.sel(time=ds.time.dt.month.isin([12,1,2]), lon=lons, lat=lats).load()\n",
    "\n",
    "    winter = spatial.resample({\"time\": \"MS\"}).sum()\n",
    "    winter = winter.assign_coords(lon=((winter[\"lon\"] + 180) % 360) - 180).sortby(\"lon\")\n",
    "\n",
    "    return winter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8c8052-62d9-4d0c-9399-4db4724b8caf",
   "metadata": {},
   "source": [
    "Load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9e180b-11ba-428c-b403-24f1e9c03862",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "historical_dss_subset_pangeo = [efficient_retrieval_and_compute_pangeo(ds) for ds in historical_dss_pangeo]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fbb71a-61c6-4c3f-8ed3-65e18a632e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ssp585_dss_subset_pangeo = [efficient_retrieval_and_compute_pangeo(ds) for ds in ssp585_dss_pangeo]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f0dda5-6dc9-48c8-be06-00f4e1fab982",
   "metadata": {},
   "source": [
    "Look at the sizes of the subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c79f664-1a54-46cf-87ed-6cdffb944c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds in historical_dss_subset_pangeo:\n",
    "    print(f'{ds[\"pr\"].size * ds[\"pr\"].dtype.itemsize / 2**20:.2f} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba57be36-ba57-435f-bb99-b565ee5c92dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds in ssp585_dss_subset_pangeo:\n",
    "    print(f'{ds[\"pr\"].size * ds[\"pr\"].dtype.itemsize / 2**20:.2f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269d8057-d588-46eb-a358-3f4828e9ff25",
   "metadata": {},
   "source": [
    "Interpolate to a common grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dde2cf9-6bf1-408e-baa9-23e711108e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssp585_dss_subset_interp_pangeo = [ds.interp(\n",
    "    lat=iad[\"lat\"],\n",
    "    lon=iad[\"lon\"],\n",
    "    method=\"linear\",\n",
    "    kwargs={\"fill_value\": \"extrapolate\"})\n",
    "              for ds in ssp585_dss_subset_pangeo]\n",
    "\n",
    "historical_dss_subset_interp_pangeo = [ds.interp(\n",
    "    lat=iad[\"lat\"],\n",
    "    lon=iad[\"lon\"],\n",
    "    method=\"linear\",\n",
    "    kwargs={\"fill_value\": \"extrapolate\"})\n",
    "              for ds in historical_dss_subset_pangeo]\n",
    "\n",
    "for ds in ssp585_dss_subset_interp_pangeo:\n",
    "    ds[\"time\"] = ds[\"time\"].astype(\"datetime64[ns]\")\n",
    "    ds[\"pr\"] = xarray.where(ds[\"pr\"] < 0, 0, ds[\"pr\"])\n",
    "\n",
    "for ds in historical_dss_subset_interp_pangeo:\n",
    "    ds[\"time\"] = ds[\"time\"].astype(\"datetime64[ns]\")\n",
    "    ds[\"pr\"] = xarray.where(ds[\"pr\"] < 0, 0, ds[\"pr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9b9786-64e7-4d5f-8637-45392600491a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmip6_ssp585_pangeo = xarray.concat(\n",
    "    ssp585_dss_subset_interp_pangeo,\n",
    "    xarray.Variable(\"member\", [ds.attrs[\"model_run\"] for ds in ssp585_dss_subset_interp_pangeo]))\n",
    "\n",
    "cmip6_historical_pangeo = xarray.concat(\n",
    "    historical_dss_subset_interp_pangeo,\n",
    "    xarray.Variable(\"member\", [ds.attrs[\"model_run\"] for ds in historical_dss_subset_interp_pangeo]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a862dfeb-7e1e-489e-b91d-1062f7a6d471",
   "metadata": {},
   "source": [
    "Reproduce the figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcdf412-351c-449d-9d1f-02d2fe88dcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmip6_ssp585_gwl = xarray.concat(\n",
    "    [cmip6_ssp585_pangeo.sel(\n",
    "        member=model_run,\n",
    "        time=slice(f\"{year-10}1201\", f\"{year+10}0201\"))\n",
    "     for model_run, year in model_run_gwl.reset_index().values\n",
    "     if year != 9999],\n",
    "    \"member\") * 2880"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7233ba-d58d-4daf-8db5-11a90c617c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "qmean = cmip6_ssp585_gwl.mean([\"lat\", \"lon\"]).resample(time=\"QS-DEC\").mean()\n",
    "plot = qmean[\"pr\"].sel(time=qmean[\"pr\"].time.dt.month==12).plot.line(x=\"time\", add_legend=False, figsize=(15, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac293bd3-7641-49a3-ac18-097ef9ee3f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lats, lons = slice(35, 72), slice(-11, 35)\n",
    "\n",
    "plot = cmip6_ssp585_gwl[\"pr\"].mean(\"time\").plot(\n",
    "    x=\"lon\", y=\"lat\", col=\"member\", col_wrap=6,\n",
    "    # vmin=0, vmax=13,\n",
    "    figsize=(28,13),\n",
    "    add_colorbar=True,\n",
    "    cmap=\"coolwarm_r\",\n",
    "    cbar_kwargs={\"shrink\": .5},\n",
    "    subplot_kws=dict(projection=ccrs.PlateCarree(central_longitude=0)),\n",
    "    transform=ccrs.PlateCarree())\n",
    "\n",
    "for ax in plot.axs.flatten():\n",
    "    ax.coastlines()\n",
    "    ax.set_extent((lons.start, lons.stop, lats.start, lats.stop), ccrs.PlateCarree())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4b2581-bdef-4bdf-bf16-3a5a0540d1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmip6_historical_gwl = xarray.concat(\n",
    "    [cmip6_historical_pangeo.sel(\n",
    "        member=model_run,\n",
    "        time=slice(\"18501201\", \"19000201\"))\n",
    "     for model_run, year in model_run_gwl.reset_index().values\n",
    "     if year != 9999],\n",
    "    \"member\") * 2880"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b9a842-b8cf-46cb-8899-25c8b3cde4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = cmip6_ssp585_gwl[\"pr\"].mean(\"time\") - cmip6_historical_gwl[\"pr\"].mean(\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a0c9cb-a5b7-435c-aafd-f5408e81d8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = delta.plot(\n",
    "    x=\"lon\", y=\"lat\", col=\"member\", col_wrap=6,\n",
    "    figsize=(28,13),\n",
    "    add_colorbar=True,\n",
    "    cmap=\"coolwarm_r\",\n",
    "    cbar_kwargs={\"shrink\": .5},\n",
    "    subplot_kws=dict(projection=ccrs.PlateCarree(central_longitude=0)),\n",
    "    transform=ccrs.PlateCarree())\n",
    "\n",
    "for ax in plot.axs.flatten():\n",
    "    ax.coastlines()\n",
    "    ax.set_extent((lons.start, lons.stop, lats.start, lats.stop), ccrs.PlateCarree())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c460e0c5-3556-407b-9cc7-281174ea6f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_rel = (delta / cmip6_historical_gwl[\"pr\"].mean(\"time\")) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c1d15d-d456-4b0a-9011-730fd727aaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = delta_rel.plot(\n",
    "    x=\"lon\", y=\"lat\", col=\"member\", col_wrap=6,\n",
    "    figsize=(28,13),\n",
    "    vmax=200, vmin=-200,\n",
    "    add_colorbar=True,\n",
    "    cmap=\"BrBG\",\n",
    "    cbar_kwargs={\"shrink\": .5},\n",
    "    subplot_kws=dict(projection=ccrs.PlateCarree(central_longitude=0)),\n",
    "    transform=ccrs.PlateCarree())\n",
    "\n",
    "for ax in plot.axs.flatten():\n",
    "    ax.coastlines()\n",
    "    ax.set_extent((lons.start, lons.stop, lats.start, lats.stop), ccrs.PlateCarree())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12994baa-c270-4910-a87e-42cb32470d92",
   "metadata": {},
   "source": [
    "Ensemble mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d70773-d729-453a-80ab-effd3e15ce10",
   "metadata": {},
   "outputs": [],
   "source": [
    "ens_mean = delta_rel.mean(\"member\")\n",
    "\n",
    "plot = ens_mean.plot(\n",
    "    add_colorbar=True,\n",
    "    cmap=\"BrBG\",\n",
    "    cbar_kwargs={\"shrink\": .5},\n",
    "    vmin=-50, vmax=50,\n",
    "    subplot_kws=dict(projection=ccrs.PlateCarree(central_longitude=0)),\n",
    "    transform=ccrs.PlateCarree())\n",
    "\n",
    "plot.axes.coastlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0465ecb0-703b-44db-8689-4ba8365f8e4f",
   "metadata": {},
   "source": [
    "Model agreement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e11bc95-b014-4f7f-9ecf-a268f841a9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_simple = delta.reduce(model_agreement, \"member\")\n",
    "\n",
    "plot = ens_mean.plot(\n",
    "    add_colorbar=True,\n",
    "    cmap=\"BrBG\",\n",
    "    cbar_kwargs={\"shrink\": .5},\n",
    "    vmin=-50, vmax=50,\n",
    "    subplot_kws=dict(projection=ccrs.PlateCarree(central_longitude=0)),\n",
    "    transform=ccrs.PlateCarree())\n",
    "\n",
    "hatching(plot, ~mask_simple, ens_mean)\n",
    "\n",
    "plot.axes.set_title(f\"Pangeo ({len(delta['member'])} models)\")\n",
    "plot.axes.coastlines()\n",
    "\n",
    "plt.savefig(\"uncer-europe-pangeo.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1ea94b-a44f-408f-9205-0618a381bf5f",
   "metadata": {},
   "source": [
    "Regional stripes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cf47c1-0888-4d3c-a37b-5b90d74330c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmip6_ssp585_y = cmip6_ssp585_gwl[\"pr\"].resample(time=\"QS-DEC\").mean()\n",
    "cmip6_ssp585_y = cmip6_ssp585_y.sel(time=cmip6_ssp585_y[\"time\"].dt.month==12)\n",
    "\n",
    "cmip6_hist_c = cmip6_historical_gwl[\"pr\"].mean([\"time\"])\n",
    "\n",
    "year_delta = cmip6_ssp585_y - cmip6_hist_c\n",
    "year_delta_rel = year_delta / cmip6_hist_c * 100\n",
    "\n",
    "regional_mean = year_delta_rel.mean([\"lat\", \"lon\"])\n",
    "\n",
    "regional_mean.plot.imshow(\n",
    "    figsize=(14,3.5),\n",
    "    add_colorbar=True,\n",
    "    cmap=\"BrBG\",\n",
    "    vmin=-50,vmax=50)\n",
    "plt.savefig(\"regional-stripes-pr.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbc5c11-23e4-4108-a90f-cf34e649903c",
   "metadata": {},
   "source": [
    "# Annex I"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394a0b16-e8b7-4428-92af-7cd1cb5cc02f",
   "metadata": {},
   "source": [
    "This functions optimizes dask chunking for efficient retrieval. However, it is even more expensive than loading unnecessary data (currently, historical takes ~13 min and with this function it takes ~23 min)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ced3ce-ffe6-4476-af99-d6ce9e1e8a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def efficient_retrieval_and_compute(ds):\n",
    "    lats, lons = slice(35, 72), slice(-11, 35) # Europe in (-180, 180)\n",
    "\n",
    "    # Step 1: Map values to group labels\n",
    "    grouped = ((ds[\"time\"].dt.month.values >= 3) & (ds[\"time\"].dt.month.values < 12)).astype(int)  # winter months 12, 1, 2\n",
    "    change_indices = np.where(np.diff(grouped) != 0)[0] + 1 # Step 2: Find where the group changes\n",
    "    segment_edges = np.concatenate(([0], change_indices, [len(ds[\"time\"].dt.month.values)])) # Step 3: Segment edges\n",
    "    counts = tuple(np.diff(segment_edges)) # Step 4: Lengths of each group run\n",
    "\n",
    "    # handle Europe being split when using (0, 360)\n",
    "    if ds[\"lon\"].min().item() >= 0:\n",
    "        left = ds.chunk({\"time\": counts}).sel(time=ds.time.dt.month.isin([12,1,2]), lon=slice(0, 36), lat=lats)\n",
    "        right = ds.chunk({\"time\": counts}).sel(time=ds.time.dt.month.isin([12,1,2]), lon=slice(348, 360), lat=lats)\n",
    "\n",
    "        spatial = xarray.concat([left, right], dim=\"lon\")\n",
    "    else:\n",
    "        spatial = ds.sel(time=ds.time.dt.month.isin([12,1,2]), lon=lons, lat=lats)\n",
    "\n",
    "    winter = spatial.resample({\"time\": \"MS\"}).sum().load()\n",
    "    winter = winter.assign_coords(lon=((winter[\"lon\"] + 180) % 360) - 180).sortby(\"lon\")\n",
    "\n",
    "    return winter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nc48",
   "language": "python",
   "name": "nc48"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
